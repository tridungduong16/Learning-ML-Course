# Matrix method for data analyst.

## The Column Space of \(A\) Contains All Vectors \(A\boldsymbol{x}\)
Section I.1: Multiplication \(A\boldsymbol{x}\) Using Columns of \(A\)


## Multiplying and Factoring Matrices 
Section I.2: Matrix-Matrix Multiplication \(AB\)


## Orthonormal Columns in \(Q\) Give \(Q’Q= I\)
Section I.5: Orthogonal Matrices and Subspaces


## Eigenvalues and Eigenvectors
Section I.6: Eigenvalues and Eigenvectors


##  Positive Definite and Semidefinite Matrices
Section I.7: Symmetric Positive Definite Matrices


## Singular Value Decomposition (SVD)
Section I.8: Singular Values and Singular Vectors in the SVD


##  Eckart-Young: The Closest Rank \(k\) Matrix to \(A\)
Section I.9: Principal Components and the Best Low Rank Matrix


## Norms of Vectors and Matrices
Section I.11: Norms of Vectors and Functions and Matrices

## Four Ways to Solve Least Squares Problems
Section II.2: Least Squares: Four Ways


## Survey of Difficulties with \(A\boldsymbol{x} = \boldsymbol{b}\)
Intro Chapter 2: Introduction to Computations with Large Matrices


## Minimizing \(‖\boldsymbol{x}‖\) Subject to \(A\boldsymbol{x} = \boldsymbol{b}\)
Section I.11: Norms of Vectors and Functions and Matrices


## Computing Eigenvalues and Singular Values
Section II.1: Numerical Linear Algebra


## Randomized Matrix Multiplication
Section II.4: Randomized Linear Algebra


## Low Rank Changes in \(A\) and Its Inverse
Section III.1: Changes in \(A^{-1}\) from Changes in \(A\)


## Matrices \(A(t)\) Depending on \(t\), Derivative = \(dA/dt\)
Section III.1: Changes in \(A^{-1}\) from Changes in \(A\)
Section III.2: Interlacing Eigenvalues and Low Rank Signals


## Derivatives of Inverse and Singular Values
Section III.1: Changes in \(A^{-1}\) from Changes in \(A\)
Section III.2: Interlacing Eigenvalues and Low Rank Signals


## Rapidly Decreasing Singular Values
Section III.3: Rapidly Decaying Singular Values


## Counting Parameters in SVD, LU, QR, Saddle Points
Section III.2: Interlacing Eigenvalues and Low Rank Signals

## Saddle Points Continued, Maxmin Principle
Section III.2: Interlacing Eigenvalues and Low Rank Signals
Section V.1: Mean, Variance, and Probability

## Definitions and Inequalities
 

## Minimizing a Function Step by Step
Section VI.1: Minimum Problems: Convexity and Newton's Method
Section VI.4: Gradient Descent Toward the Minimum

## Gradient Descent: Downhill to a Minimum
Section VI.4: Gradient Descent Toward the Minimum

## Accelerating Gradient Descent (Use Momentum)
Section VI.4: Gradient Descent Toward the Minimum

## Linear Programming and Two-Person Games
Section VI.2: Lagrange Multipliers = Derivatives of the Cost
Section VI.3: Linear Programming, Game Theory, and Duality

## Stochastic Gradient Descent
Section VI.5: Stochastic Gradient Descent and ADAM

## Structure of Neural Nets for Deep Learning
Section VII.1: The Construction of Deep Neural Networks

## Backpropagation: Find Partial Derivatives
Section VII.3: Backpropagation and the Chain Rule

## Computing in Class [No video available]
Section VII.2: Convolutional Neural Nets

##  Computing in Class (cont.) [No video available]
 

## Completing a Rank One Matrix, Circulants!
Section IV.8: Completing Rank One Matrices
Section IV.2: Shift Matrices and Circulant Matrices

## Eigenvectors of Circulant Matrices: Fourier Matrix
Section IV.2: Shift Matrices and Circulant Matrices

## ImageNet is a Convolutional Neural Network (CNN), The Convolution Rule
Section IV.2: Shift Matrices and Circulant Matrices

##  Neural Nets and the Learning Function
Section VII.1: The Construction of Deep Neural Networks 
Section IV.10: Distance Matrices

##  Distance Matrices, Procrustes Problem
Section IV.9: The Orthogonal Procrustes Problem
Section IV.10: Distance Matrices
## Finding Clusters in Graphs
Section IV.6: Graphs and Laplacians and Kirchhoff's Laws
Section IV.7: Clustering by Spectral Methods and \(k\)-means

## Alan Edelman and Julia Language
Section III.3: Rapidly Decaying Singular Values
Section VII.2: Convolutional Neural Nets
